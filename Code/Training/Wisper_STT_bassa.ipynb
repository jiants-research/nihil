{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1254d06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whisper Fine-Tuning sur Common Voice Bassa\n",
    "\n",
    "## 1. Configuration\n",
    "\n",
    "from datasets import load_dataset, Audio\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, TrainingArguments, Trainer\n",
    "import torchaudio\n",
    "import torch\n",
    "import pandas as pd\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d807d814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      audio_path                    sentence\n",
      "0  Bassa\\clips_wav\\common_voice_bas_41203802.wav                     T√¥s nu.\n",
      "1  Bassa\\clips_wav\\common_voice_bas_41203803.wav              Mbas i nhamba.\n",
      "2  Bassa\\clips_wav\\common_voice_bas_41203804.wav  Ba ntip babaa hiloga hini.\n",
      "3  Bassa\\clips_wav\\common_voice_bas_41203806.wav      A nl√¥m e; a nl√¥m hy√©√©.\n",
      "4  Bassa\\clips_wav\\common_voice_bas_41203812.wav    Ba njul b√¥t i k√©d√© hisi.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "## 2. Chargement du corpus Common Voice Bassa (apr√®s extraction)\n",
    "\n",
    "\n",
    "# Dossiers\n",
    "data_dir = \"Bassa\"\n",
    "clips_wav_dir = os.path.join(data_dir, \"clips_wav\")\n",
    "\n",
    "# Chargement des TSV\n",
    "train_df = pd.read_csv(os.path.join(data_dir, \"train.tsv\"), sep=\"\\t\")\n",
    "dev_df = pd.read_csv(os.path.join(data_dir, \"dev.tsv\"), sep=\"\\t\")\n",
    "test_df = pd.read_csv(os.path.join(data_dir, \"test.tsv\"), sep=\"\\t\")\n",
    "\n",
    "# Modifier les chemins pour utiliser les .wav au lieu des .mp3\n",
    "for df in [train_df, dev_df, test_df]:\n",
    "    df['audio_path'] = df['path'].apply(\n",
    "        lambda p: os.path.join(clips_wav_dir, p.replace('.mp3', '.wav'))\n",
    "    )\n",
    "\n",
    "# V√©rification\n",
    "print(train_df[['audio_path', 'sentence']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5f6eac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train ‚Äî Fichiers audio manquants : 0\n",
      "dev ‚Äî Fichiers audio manquants : 0\n",
      "test ‚Äî Fichiers audio manquants : 0\n"
     ]
    }
   ],
   "source": [
    "# V√©rifier si les fichiers .wav sont bien l√†\n",
    "for name, df in zip(['train', 'dev', 'test'], [train_df, dev_df, test_df]):\n",
    "    missing = df[~df['audio_path'].apply(os.path.exists)]\n",
    "    print(f\"{name} ‚Äî Fichiers audio manquants : {len(missing)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5609612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 53568]) 16000\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "\n",
    "# Exemple : charger un fichier wav\n",
    "waveform, sample_rate = torchaudio.load(train_df.loc[0, 'audio_path'])\n",
    "print(waveform.shape, sample_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ee66b4",
   "metadata": {},
   "source": [
    "# Whisper Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a60a2dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\GENIUS ELECTRONICS\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "## 1. Configuration\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "from datasets import Dataset\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, TrainingArguments, Trainer\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0d3d588",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## 2. Chemins d'acc√®s aux donn√©es\n",
    "\n",
    "\n",
    "data_dir = \"Bassa\"\n",
    "clips_wav_dir = os.path.join(data_dir, \"clips_wav\")\n",
    "\n",
    "# Charger les fichiers TSV\n",
    "def load_split(split_name):\n",
    "    df = pd.read_csv(os.path.join(data_dir, f\"{split_name}.tsv\"), sep=\"\\t\")\n",
    "    df = df[df['sentence'].notnull()]  # enlever les phrases manquantes\n",
    "    df['audio_path'] = df['path'].apply(lambda p: os.path.join(clips_wav_dir, p.replace('.mp3', '.wav')))\n",
    "    return df\n",
    "\n",
    "train_df = load_split(\"train\")\n",
    "dev_df = load_split(\"dev\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52dccf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## 3. Cr√©ation du Dataset compatible Hugging Face\n",
    "\n",
    "\n",
    "def df_to_dataset(df):\n",
    "    return Dataset.from_pandas(df[['audio_path', 'sentence']])\n",
    "\n",
    "train_dataset = df_to_dataset(train_df)\n",
    "dev_dataset = df_to_dataset(dev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eef51f82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2109, 2), (1328, 2))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.shape, dev_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3aed145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Appliquer le pr√©traitement\\ntrain_dataset = train_dataset.map(preprocess, remove_columns=train_dataset.column_names)\\ndev_dataset = dev_dataset.map(preprocess, remove_columns=dev_dataset.column_names)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 4. Traitement audio et texte pour Whisper\n",
    "\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\")\n",
    "\n",
    "\n",
    "def preprocess(example):\n",
    "    # Charger et resampler l'audio\n",
    "    speech_array, sampling_rate = torchaudio.load(example['audio_path'])\n",
    "    if sampling_rate != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sampling_rate, new_freq=16000)\n",
    "        speech_array = resampler(speech_array)\n",
    "\n",
    "    # Extraire les features audio\n",
    "    inputs = processor(\n",
    "        speech_array.squeeze().numpy(),\n",
    "        sampling_rate=16000,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Extraire les labels texte\n",
    "    labels = processor.tokenizer(\n",
    "        example[\"sentence\"],\n",
    "        return_tensors=\"pt\",\n",
    "        padding=False,\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"input_features\": inputs.input_features[0],  # torch.Tensor\n",
    "        \"labels\": labels.input_ids[0]               # torch.Tensor\n",
    "    }\n",
    "\n",
    "\n",
    "'''\n",
    "# Appliquer le pr√©traitement\n",
    "train_dataset = train_dataset.map(preprocess, remove_columns=train_dataset.column_names)\n",
    "dev_dataset = dev_dataset.map(preprocess, remove_columns=dev_dataset.column_names)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98120263",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. Chargement du mod√®le\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\")\n",
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f0d9b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.53.0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2fa8fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def whisper_data_collator(features):\n",
    "    audio_paths = [f[\"audio_path\"] for f in features]\n",
    "    texts = [f[\"sentence\"] for f in features]\n",
    "\n",
    "    input_features = []\n",
    "    for path in audio_paths:\n",
    "        speech_array, sr = torchaudio.load(path)\n",
    "        if sr != 16000:\n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)\n",
    "            speech_array = resampler(speech_array)\n",
    "        processed = processor(\n",
    "            speech_array.squeeze().numpy(),\n",
    "            sampling_rate=16000,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_features.append(processed.input_features[0])\n",
    "\n",
    "    batch = processor.feature_extractor.pad(\n",
    "        {\"input_features\": input_features},\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    label_batch = processor.tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True\n",
    "    )\n",
    "    labels = label_batch[\"input_ids\"].masked_fill(label_batch[\"attention_mask\"].ne(1), -100)\n",
    "\n",
    "    batch[\"labels\"] = labels\n",
    "\n",
    "    #print(\"üö® batch keys:\", batch.keys())\n",
    "    #print(\"‚úÖ input_features shape:\", batch[\"input_features\"].shape)\n",
    "    #print(\"‚úÖ labels shape:\", labels.shape)\n",
    "\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b45b160f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in train_dataset:\n",
    "    if row[\"sentence\"] is None or row[\"sentence\"] == \"\":\n",
    "        print(\"‚ùå Texte vide :\", row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c15cc6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter:   0%|          | 0/2109 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2109/2109 [00:00<00:00, 30724.35 examples/s]\n",
      "Filter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1328/1328 [00:00<00:00, 142905.71 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset.filter(lambda x: x[\"sentence\"] is not None and x[\"sentence\"] != \"\")\n",
    "dev_dataset = dev_dataset.filter(lambda x: x[\"sentence\"] is not None and x[\"sentence\"] != \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87b012a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GENIUS ELECTRONICS\\AppData\\Local\\Temp\\ipykernel_19952\\470880417.py:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "## 6. Entra√Ænement avec HuggingFace Trainer\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./whisper-bassa-model\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=1e-4,\n",
    "    logging_dir=\"./logs\",\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from jiwer import wer, cer\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    return {\"wer\": wer(label_str, pred_str), \"cer\": cer(label_str, pred_str)}\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    tokenizer=None,\n",
    "    data_collator=whisper_data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6eedb421",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['proj_out.weight'].\n",
      "c:\\Users\\GENIUS ELECTRONICS\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5280' max='5280' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5280/5280 3:52:20, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.060500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.033600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.014800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.004100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.000800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\GENIUS ELECTRONICS\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\modeling_utils.py:3685: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 448, 'suppress_tokens': [], 'begin_suppress_tokens': [220, 50257]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "c:\\Users\\GENIUS ELECTRONICS\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\GENIUS ELECTRONICS\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\GENIUS ELECTRONICS\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\GENIUS ELECTRONICS\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5280, training_loss=0.010810695371280115, metrics={'train_runtime': 13946.9808, 'train_samples_per_second': 1.512, 'train_steps_per_second': 0.379, 'total_flos': 5.192122871808e+17, 'train_loss': 0.010810695371280115, 'epoch': 10.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 7. Lancer l'entra√Ænement\n",
    "trainer.train(resume_from_checkpoint=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edf20c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## 8. √âvaluer le mod√®le\n",
    "metrics = trainer.evaluate()\n",
    "#trainer.evaluate(eval_dataset=dev_dataset)\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d56fd4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3449fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/664 [00:00<?, ?it/s]Using custom `forced_decoder_ids` from the (generation) config. This is deprecated in favor of the `task` and `language` flags/config options.\n",
      "Transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English. This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`. See https://github.com/huggingface/transformers/pull/28687 for more details.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 664/664 [22:19<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ WER sur dev_dataset : {'wer': 0.5071082879612825}\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLe noyau s‚Äôest bloqu√© lors de l‚Äôex√©cution du code dans une cellule active ou une cellule pr√©c√©dente. \n",
      "\u001b[1;31mVeuillez v√©rifier le code dans la ou les cellules pour identifier une cause possible de l‚Äô√©chec. \n",
      "\u001b[1;31mCliquez <a href='https://aka.ms/vscodeJupyterKernelCrash'>ici</a> pour plus d‚Äôinformations. \n",
      "\u001b[1;31mPour plus d‚Äôinformations, consultez Jupyter <a href='command:jupyter.viewOutput'>log</a>."
     ]
    }
   ],
   "source": [
    "from jiwer import wer\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_in_batches(dataset, batch_size=4):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for i in tqdm(range(0, len(dataset), batch_size)):\n",
    "        batch = dataset[i:i+batch_size]\n",
    "        input_features = []\n",
    "        labels_text = []\n",
    "\n",
    "        for j in range(len(batch[\"audio_path\"])):\n",
    "            path = batch[\"audio_path\"][j]\n",
    "            text = batch[\"sentence\"][j]\n",
    "\n",
    "            speech_array, sr = torchaudio.load(path)\n",
    "            if sr != 16000:\n",
    "                resampler = torchaudio.transforms.Resample(sr, 16000)\n",
    "                speech_array = resampler(speech_array)\n",
    "\n",
    "            inputs = processor(speech_array.squeeze().numpy(), sampling_rate=16000, return_tensors=\"pt\")\n",
    "            input_features.append(inputs.input_features[0])\n",
    "            labels_text.append(text)\n",
    "\n",
    "        batch_input = processor.feature_extractor.pad({\"input_features\": input_features}, return_tensors=\"pt\")\n",
    "        input_tensor = batch_input.input_features.to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predicted_ids = model.generate(input_tensor)\n",
    "\n",
    "        pred_str = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "        all_preds.extend(pred_str)\n",
    "        all_labels.extend(labels_text)\n",
    "\n",
    "    return {\"wer\": wer(all_labels, all_preds)}\n",
    "\n",
    "\n",
    "# ‚û§ √âvaluation\n",
    "metrics = evaluate_in_batches(dev_dataset, batch_size=2)\n",
    "print(\"‚úÖ WER sur dev_dataset :\", metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13cbec10",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 9. Sauvegarde du mod√®le\n",
    "trainer.save_model(\"./API Model/model/model_final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0094d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(audio_path):\n",
    "    speech_array, sr = torchaudio.load(audio_path)\n",
    "    if sr != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)\n",
    "        speech_array = resampler(speech_array)\n",
    "    \n",
    "    inputs = processor(speech_array.squeeze().numpy(), sampling_rate=16000, return_tensors=\"pt\")\n",
    "    input_features = inputs.input_features.to(model.device)\n",
    "    with torch.no_grad():\n",
    "        predicted_ids = model.generate(input_features)\n",
    "    return processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "# Exemple d'utilisation :\n",
    "transcribe_audio(\"C:/Users/GENIUS ELECTRONICS/STT-Bassa/Bassa/test/test1.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4340818b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\GENIUS ELECTRONICS\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a5e473",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
